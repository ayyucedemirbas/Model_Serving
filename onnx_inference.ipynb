{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOO+4gbEEbxkLl3aDZB3BGo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayyucedemirbas/Model_Serving/blob/main/onnx_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ayyucedemirbas/onnxruntime-nextjs-template.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GTlD9Dsvj8c",
        "outputId": "46c90ed9-42dc-4240-dbdc-f3fd6b84d6cf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'onnxruntime-nextjs-template'...\n",
            "remote: Enumerating objects: 112, done.\u001b[K\n",
            "remote: Total 112 (delta 0), reused 0 (delta 0), pack-reused 112\u001b[K\n",
            "Receiving objects: 100% (112/112), 107.19 MiB | 29.21 MiB/s, done.\n",
            "Resolving deltas: 100% (41/41), done.\n",
            "Updating files: 100% (27/27), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/onnxruntime-nextjs-template/notebook"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn0Oo0WZvtiR",
        "outputId": "8c89e977-a9bf-4e74-c23a-c7fd70660669"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/onnxruntime-nextjs-template/notebook\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ZacQpO0Avfv9",
        "outputId": "a7fc07f1-4bed-456d-b9f8-5d04db0d7225"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "import {InferenceSession, Tensor} from 'onnxruntime-web';\n",
              " const ndarray = require('ndarray')\n",
              " const ops = require('ndarray-ops')\n",
              " const fs = require('fs')\n",
              " const jimp = require('jimp')\n",
              " var text = fs.readFileSync(\"classes.txt\").toString('utf-8');\n",
              " const classes = text.split(\"\\r\\n\");\n",
              "var path = 'https://farm2.staticflickr.com/1533/26541536141_41abe98db3_z_d.jpg'\n",
              "var imageData = null;\n",
              "var imageData = await jimp.read(path).then(image => {\n",
              "    return image.resize(224, 224) // resize\n",
              "    //console.log(imageData.bitmap)\n",
              "      //.quality(60) // set JPEG quality\n",
              "      //.greyscale() // set greyscale\n",
              "      //.write('./data/bird-small-bw.jpg'); // save\n",
              "  })\n",
              "  .catch(err => {\n",
              "    console.error(err);\n",
              "  });\n",
              "function imageDataToTensor(data, dims): any {\n",
              "    // 1a. Extract the R, G, and B channels from the data to form a 3D int array\n",
              "    const [R, G, B] = new Array([], [], []);\n",
              "    for (let i = 0; i < data.length; i += 4) {\n",
              "      R.push(data[i]);\n",
              "      G.push(data[i + 1]);\n",
              "      B.push(data[i + 2]);\n",
              "      // 2. skip data[i + 3] thus filtering out the alpha channel\n",
              "    }\n",
              "    ///console.log(R);\n",
              "    //console.log(G);\n",
              "    //console.log(B);\n",
              "    // 1b. concatenate RGB ~= transpose [224, 224, 3] -> [3, 224, 224]\n",
              "    const transposedData = R.concat(G).concat(B);\n",
              "\n",
              "    // 3. convert to float32\n",
              "    let i, l = transposedData.length; // length, we need this for the loop\n",
              "    const float32Data = new Float32Array(3 * 224 * 224); // create the Float32Array for output\n",
              "    for (i = 0; i < l; i++) {\n",
              "      float32Data[i] = transposedData[i] / 255.0; // convert to float\n",
              "    }\n",
              "  \n",
              "    const inputTensor = new Tensor(\"float32\", float32Data, dims);\n",
              "    return inputTensor;\n",
              "  }\n",
              "var data = imageDataToTensor(imageData.bitmap.data, [1, 3, 224, 224])\n",
              "// create an inference session, using WebGL backend. (default is 'wasm') \n",
              "//const session = await ort.InferenceSession.create('./model/squeezenet1_1.onnx', { executionProviders: ['wasm'] }); \n",
              "const session = await InferenceSession.create('../model/resnet50v2.onnx', { executionProviders: ['wasm'] });\n",
              "async function runModel(model, preprocessedData): Promise<[Tensor, number]> {\n",
              "    const start = new Date();\n",
              "    try {\n",
              "      const feeds: Record<string, Tensor> = {};\n",
              "      feeds[model.inputNames[0]] = preprocessedData;\n",
              "      const outputData = await model.run(feeds);\n",
              "      const end = new Date();\n",
              "      const inferenceTime = (end.getTime() - start.getTime());\n",
              "      const output = outputData[model.outputNames[0]];\n",
              "      return [output, inferenceTime];\n",
              "    } catch (e) {\n",
              "      console.error(e);\n",
              "      throw new Error();\n",
              "    }\n",
              "  }\n",
              "//The softmax transforms values to be between 0 and 1\n",
              "function softmax(resultArray: number[]): any {\n",
              "  // Get the largest value in the array.\n",
              "  const largestNumber = Math.max(...resultArray);\n",
              "  // Apply exponential function to each result item subtracted by the largest number, use reduce to get the previous result number and the current number to sum all the exponentials results.\n",
              "  const sumOfExp = resultArray.map((resultItem) => Math.exp(resultItem - largestNumber)).reduce((prevNumber, currentNumber) => prevNumber + currentNumber);\n",
              "  //Normalizes the resultArray by dividing by the sum of all exponentials; this normalization ensures that the sum of the components of the output vector is 1.\n",
              "  return resultArray.map((resultValue, index) => {\n",
              "    return Math.exp(resultValue - largestNumber) / sumOfExp;\n",
              "  });\n",
              "}\n",
              "const [res, time] =  await runModel(session, data);\n",
              "var output = res.data;\n",
              "var inferenceTime = time;\n",
              "var results = softmax(Array.prototype.slice.call(output))\n",
              "var topResults = [];\n",
              "for (let i = 0; i < results.length; i++) {\n",
              "  if (results[i] > 0.3) {\n",
              "    topResults.push([classes[i] + \": \" + results[i]]);\n",
              "  }\n",
              "}\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "%%javascript\n",
        "import {InferenceSession, Tensor} from 'onnxruntime-web';\n",
        " const ndarray = require('ndarray')\n",
        " const ops = require('ndarray-ops')\n",
        " const fs = require('fs')\n",
        " const jimp = require('jimp')\n",
        " var text = fs.readFileSync(\"classes.txt\").toString('utf-8');\n",
        " const classes = text.split(\"\\r\\n\");\n",
        "var path = 'https://farm2.staticflickr.com/1533/26541536141_41abe98db3_z_d.jpg'\n",
        "var imageData = null;\n",
        "var imageData = await jimp.read(path).then(image => {\n",
        "    return image.resize(224, 224) // resize\n",
        "    //console.log(imageData.bitmap)\n",
        "      //.quality(60) // set JPEG quality\n",
        "      //.greyscale() // set greyscale\n",
        "      //.write('./data/bird-small-bw.jpg'); // save\n",
        "  })\n",
        "  .catch(err => {\n",
        "    console.error(err);\n",
        "  });\n",
        "function imageDataToTensor(data, dims): any {\n",
        "    // 1a. Extract the R, G, and B channels from the data to form a 3D int array\n",
        "    const [R, G, B] = new Array([], [], []);\n",
        "    for (let i = 0; i < data.length; i += 4) {\n",
        "      R.push(data[i]);\n",
        "      G.push(data[i + 1]);\n",
        "      B.push(data[i + 2]);\n",
        "      // 2. skip data[i + 3] thus filtering out the alpha channel\n",
        "    }\n",
        "    ///console.log(R);\n",
        "    //console.log(G);\n",
        "    //console.log(B);\n",
        "    // 1b. concatenate RGB ~= transpose [224, 224, 3] -> [3, 224, 224]\n",
        "    const transposedData = R.concat(G).concat(B);\n",
        "\n",
        "    // 3. convert to float32\n",
        "    let i, l = transposedData.length; // length, we need this for the loop\n",
        "    const float32Data = new Float32Array(3 * 224 * 224); // create the Float32Array for output\n",
        "    for (i = 0; i < l; i++) {\n",
        "      float32Data[i] = transposedData[i] / 255.0; // convert to float\n",
        "    }\n",
        "  \n",
        "    const inputTensor = new Tensor(\"float32\", float32Data, dims);\n",
        "    return inputTensor;\n",
        "  }\n",
        "var data = imageDataToTensor(imageData.bitmap.data, [1, 3, 224, 224])\n",
        "// create an inference session, using WebGL backend. (default is 'wasm') \n",
        "//const session = await ort.InferenceSession.create('./model/squeezenet1_1.onnx', { executionProviders: ['wasm'] }); \n",
        "const session = await InferenceSession.create('../model/resnet50v2.onnx', { executionProviders: ['wasm'] });\n",
        "async function runModel(model, preprocessedData): Promise<[Tensor, number]> {\n",
        "    const start = new Date();\n",
        "    try {\n",
        "      const feeds: Record<string, Tensor> = {};\n",
        "      feeds[model.inputNames[0]] = preprocessedData;\n",
        "      const outputData = await model.run(feeds);\n",
        "      const end = new Date();\n",
        "      const inferenceTime = (end.getTime() - start.getTime());\n",
        "      const output = outputData[model.outputNames[0]];\n",
        "      return [output, inferenceTime];\n",
        "    } catch (e) {\n",
        "      console.error(e);\n",
        "      throw new Error();\n",
        "    }\n",
        "  }\n",
        "//The softmax transforms values to be between 0 and 1\n",
        "function softmax(resultArray: number[]): any {\n",
        "  // Get the largest value in the array.\n",
        "  const largestNumber = Math.max(...resultArray);\n",
        "  // Apply exponential function to each result item subtracted by the largest number, use reduce to get the previous result number and the current number to sum all the exponentials results.\n",
        "  const sumOfExp = resultArray.map((resultItem) => Math.exp(resultItem - largestNumber)).reduce((prevNumber, currentNumber) => prevNumber + currentNumber);\n",
        "  //Normalizes the resultArray by dividing by the sum of all exponentials; this normalization ensures that the sum of the components of the output vector is 1.\n",
        "  return resultArray.map((resultValue, index) => {\n",
        "    return Math.exp(resultValue - largestNumber) / sumOfExp;\n",
        "  });\n",
        "}\n",
        "const [res, time] =  await runModel(session, data);\n",
        "var output = res.data;\n",
        "var inferenceTime = time;\n",
        "var results = softmax(Array.prototype.slice.call(output))\n",
        "var topResults = [];\n",
        "for (let i = 0; i < results.length; i++) {\n",
        "  if (results[i] > 0.3) {\n",
        "    topResults.push([classes[i] + \": \" + results[i]]);\n",
        "  }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%javascript\n",
        "console.log(topResults);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5r3hDimtv8h6",
        "outputId": "f09e9f1f-eb05-4d48-ece8-57d3313f00f6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "console.log(topResults);\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDGYvZTbwzDo",
        "outputId": "6fb0b975-ed10-42cb-c2c2-ff066df6cd4f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/onnxruntime-nextjs-template/notebook\n"
          ]
        }
      ]
    }
  ]
}